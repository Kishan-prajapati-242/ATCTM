{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN0dFktc40EvhLTbSP1ZVxT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kishan-prajapati-242/ATCTM/blob/main/notebooks/EC_demo_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsjljGRSYEGF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch import nn\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "from textblob import TextBlob\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/ATCTM/EVENT_CLASSIFICATION/EC-demo.csv')\n",
        "df.dropna(subset=['TEXT'], inplace=True)\n",
        "\n",
        "# Encode categorical columns\n",
        "le_event = LabelEncoder()\n",
        "le_emotion = LabelEncoder()\n",
        "le_tense = LabelEncoder()\n",
        "\n",
        "# Encoding target labels\n",
        "df['EVENT_TYPE_ID'] = le_event.fit_transform(df['EVENT_TYPE'])\n",
        "df['EMOTION_ID'] = le_emotion.fit_transform(df['EMOTION'])\n",
        "df['TENSE_ID'] = le_tense.fit_transform(df['TENSE'])\n",
        "df['SARCASM_ID'] = df['SARCASM'].astype(int)\n",
        "\n",
        "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "for train_idx, val_idx in splitter.split(df, df['EVENT_TYPE_ID']):\n",
        "    train_df = df.iloc[train_idx].reset_index(drop=True)\n",
        "    val_df = df.iloc[val_idx].reset_index(drop=True)\n",
        "\n",
        "\n",
        "# Dataset Class\n",
        "class EventDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len=128):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        encoded = self.tokenizer(row['TEXT'], padding='max_length', truncation=True, max_length=self.max_len, return_tensors='pt')\n",
        "        return {\n",
        "            'input_ids': encoded['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoded['attention_mask'].squeeze(0),\n",
        "            'event_type': torch.tensor(row['EVENT_TYPE_ID']),\n",
        "            'emotion': torch.tensor(row['EMOTION_ID']),\n",
        "            'sarcasm': torch.tensor(row['SARCASM_ID']),\n",
        "            'tense': torch.tensor(row['TENSE_ID']),\n",
        "        }\n",
        "\n",
        "# Model Definition\n",
        "class EventClassifier(nn.Module):\n",
        "    def __init__(self, num_event_types, num_emotions, num_tenses):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.shared = nn.Linear(768, 512)\n",
        "\n",
        "        self.event_proj = nn.Linear(512, 256)\n",
        "        self.emotion_proj = nn.Linear(512, 256)\n",
        "        self.sarcasm_proj = nn.Linear(512, 256)\n",
        "        self.tense_proj = nn.Linear(512, 256)\n",
        "\n",
        "        self.event_head = nn.Linear(256, num_event_types)\n",
        "        self.emotion_head = nn.Linear(256, num_emotions)\n",
        "        self.sarcasm_head = nn.Linear(256, 1)\n",
        "        self.tense_head = nn.Linear(256, num_tenses)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        x = self.dropout(output.pooler_output)\n",
        "        x = self.shared(x)\n",
        "\n",
        "        return {\n",
        "            'event_type': self.event_head(self.event_proj(x)),\n",
        "            'emotion': self.emotion_head(self.emotion_proj(x)),\n",
        "            'sarcasm': torch.sigmoid(self.sarcasm_head(self.sarcasm_proj(x))),\n",
        "            'tense': self.tense_head(self.tense_proj(x))\n",
        "        }\n",
        "\n",
        "# Tokenizer and DataLoader\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "train_data = EventDataset(train_df, tokenizer)\n",
        "val_data = EventDataset(val_df, tokenizer)\n",
        "train_loader = DataLoader(train_data, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=8)\n",
        "\n",
        "# Emotion class weights\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "weights = compute_class_weight('balanced', classes=np.unique(df['EMOTION_ID']), y=df['EMOTION_ID'])\n",
        "emotion_weights = torch.tensor(weights, dtype=torch.float)\n",
        "\n",
        "# Setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = EventClassifier(len(le_event.classes_), len(le_emotion.classes_), len(le_tense.classes_)).to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "loss_fn_ce = nn.CrossEntropyLoss()\n",
        "loss_fn_emo = nn.CrossEntropyLoss(weight=emotion_weights.to(device))\n",
        "loss_fn_bce = nn.BCELoss()\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(4):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_loader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "\n",
        "        loss_event = loss_fn_ce(outputs['event_type'], batch['event_type'].to(device))\n",
        "        loss_emo = loss_fn_emo(outputs['emotion'], batch['emotion'].to(device))\n",
        "        loss_sar = loss_fn_bce(outputs['sarcasm'].squeeze(), batch['sarcasm'].float().to(device))\n",
        "        loss_tense = loss_fn_ce(outputs['tense'], batch['tense'].to(device))\n",
        "\n",
        "        loss = loss_event + loss_emo + loss_sar + loss_tense\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}: Loss = {total_loss:.4f}\")"
      ],
      "metadata": {
        "id": "7gdFN22MYVrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Evaluation (Event Type only)\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "\n",
        "        all_preds += outputs['event_type'].argmax(dim=1).cpu().tolist()\n",
        "        all_labels += batch['event_type'].tolist()\n",
        "\n",
        "print(\"\\nEvent Type Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=le_event.classes_))\n",
        "\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f\"\\nEvent Type Accuracy: {accuracy * 100:.2f}% out of 100\\n\")\n",
        "\n",
        "# Utility Functions\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    score = sid.polarity_scores(text)['compound']\n",
        "    normalized = round((score + 1) / 2, 1)\n",
        "    return max(min(normalized, 1.0), 0.0)\n",
        "\n",
        "def analyze_certainty(text):\n",
        "    certainty_keywords = [\"sure\", \"definitely\", \"certain\", \"guarantee\", \"confident\", \"no doubt\"]\n",
        "    fuzzy_keywords = [\"maybe\", \"possibly\", \"might\", \"not sure\", \"doubt\"]\n",
        "    text = text.lower()\n",
        "    if any(w in text for w in certainty_keywords):\n",
        "        return 1.0\n",
        "    elif any(w in text for w in fuzzy_keywords):\n",
        "        return 0.3\n",
        "    return 0.6\n",
        "\n",
        "# Inference\n",
        "def predict_event(text):\n",
        "    model.eval()\n",
        "    encoded = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=128)\n",
        "    input_ids = encoded['input_ids'].to(device)\n",
        "    attention_mask = encoded['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "\n",
        "    pred_event = le_event.inverse_transform([torch.argmax(outputs['event_type'], dim=1).item()])[0]\n",
        "    pred_emotion = le_emotion.inverse_transform([torch.argmax(outputs['emotion'], dim=1).item()])[0]\n",
        "    pred_sarcasm = outputs['sarcasm'].item() > 0.5\n",
        "    pred_tense = le_tense.inverse_transform([torch.argmax(outputs['tense'], dim=1).item()])[0]\n",
        "\n",
        "    return {\n",
        "        \"TEXT\": text,\n",
        "        \"EVENT_TYPE\": pred_event,\n",
        "        \"EVENT_GROUP\": \"employment\",\n",
        "        \"SENTIMENT_VALENCE\": analyze_sentiment(text),\n",
        "        \"EMOTION\": pred_emotion,\n",
        "        \"SARCASM\": pred_sarcasm,\n",
        "        \"TENSE\": pred_tense,\n",
        "        \"CERTAINTY\": analyze_certainty(text)\n",
        "    }\n",
        "\n",
        "# Example Usage\n",
        "print(predict_event(\"They finally laid me off after months of warnings.\"))\n",
        "print(predict_event(\"Just got hired at a startup in Berlin. I'm thrilled!\"))"
      ],
      "metadata": {
        "id": "kZLX30IubbvB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}